From 1099240aaf86722d541e4f2c9a5a6b06b11eb128 Mon Sep 17 00:00:00 2001
From: Noah Huetter <noahhuetter@gmail.com>
Date: Tue, 11 Jan 2022 13:54:05 +0100
Subject: [sw/snitch] Add AXPY kernel

Signed-off-by: Noah Huetter <noahhuetter@gmail.com>
---
 sw/snBLAS/include/axpy.h |  41 ++++++++++
 sw/snBLAS/src/axpy.c     | 194 ++++++++++++++++++++++++++++++++++++++++++++++-
 2 files changed, 234 insertions(+), 1 deletion(-)
 create mode 100644 sw/vendor/snitch/sw/snBLAS/include/axpy.h

diff --git a/sw/snBLAS/include/axpy.h b/sw/snBLAS/include/axpy.h
new file mode 100644
index 0000000..ddcd3d9
--- /dev/null
+++ b/sw/snBLAS/include/axpy.h
@@ -0,0 +1,41 @@
+// Copyright 2022 ETH Zurich and University of Bologna.
+// Licensed under the Apache License, Version 2.0, see LICENSE for details.
+// SPDX-License-Identifier: Apache-2.0
+#pragma once
+
+#include <snblas.h>
+#include <stdint.h>
+
+/**
+ * @brief Double-buffering AXPY. TCDM footprint ~= 32*tile_n*sizeof(double)
+ * @details dy[i] = dy[i] + alpha*dx[i]
+ *
+ * @param n Problem size
+ * @param tile_n Tiling size
+ * @param alpha pointer to alpga
+ * @param dx pointer to data x
+ * @param dy pointer to data y
+ * @param ccfg compute configuration struct
+ */
+void axpy_block(uint32_t n, uint32_t tile_n, double *alpha, double *dx, double *dy,
+                computeConfig_t *ccfg);
+
+/**
+ * @brief DAXPY constant times a vector plus a vector.
+ * @details dy[i] = dy[i] + alpha*dx[i]
+ *
+ * @param n number of elements to process
+ * @param alpha scalar alpha
+ * @param dx double array
+ * @param dy double array
+ * @param setup_ssr Set to non-zero if the problem size changed since the last call or the SSRs have
+ * been used with a different configuration. Set to zero for faster execution
+ */
+void axpy(uint32_t n, double *alpha, double *dx, double *dy, uint32_t setup_ssr);
+
+/**
+ * @brief Test `axpy` and return number of mismatches
+ *
+ * @return Number of mismatches
+ */
+unsigned test_axpy(void);
diff --git a/sw/snBLAS/src/axpy.c b/sw/snBLAS/src/axpy.c
index cf54ead..9738783 100644
--- a/sw/snBLAS/src/axpy.c
+++ b/sw/snBLAS/src/axpy.c
@@ -1,6 +1,198 @@
 // Copyright 2020 ETH Zurich and University of Bologna.
 // Licensed under the Apache License, Version 2.0, see LICENSE for details.
 // SPDX-License-Identifier: Apache-2.0
+#include "axpy.h"
+#include "printf.h"
 #include <snblas.h>
+#include <snrt.h>
 
-double snblas_hello() { return 42.3141; }
+void axpy_block(uint32_t n, uint32_t tile_n, double *alpha, double *dx, double *dy,
+                computeConfig_t *ccfg) {
+  uint32_t compute_id = snrt_cluster_compute_core_idx();
+  uint32_t compute_num = snrt_cluster_compute_core_num();
+  const size_t bpw = sizeof(double);
+  // padding to mis-align between buffers for each hart to reduce TCDM congestion
+  const size_t padding = 1;
+  uint32_t ni = 0;
+  uint32_t cs = 0;
+  int waiter = 0;
+  uint64_t t1, t2, bsel = 0;
+
+  // allocate data
+  const size_t tile_stride = tile_n / compute_num + padding;
+  const size_t buf_stride = compute_num * tile_stride;
+  double *ptr = (double *)snrt_cluster_memory().start;
+  double *l1_dx = ptr;
+  ptr += 2 * buf_stride;
+  double *l1_dy = ptr;
+  ptr += 2 * buf_stride;
+  double *l1_alpha = ptr;
+  ptr += 1;
+
+  // ------------------
+  //   Data mover
+  // ------------------
+
+  if (snrt_is_dm_core()) {
+    // initial copy-in
+    snrt_dma_start_1d(l1_alpha, alpha, bpw * 1);
+    snrt_dma_start_2d(&l1_dx[bsel * buf_stride],             /* dst */
+                      &dx[ni],                               /* src */
+                      bpw * tile_n / compute_num,            /* size */
+                      sizeof(double) * tile_stride,          /* dst_stride */
+                      sizeof(double) * tile_n / compute_num, /* src_stride */
+                      compute_num /* repetitions */);
+    snrt_dma_start_2d(&l1_dy[bsel * buf_stride], &dy[ni], bpw * tile_n / compute_num,
+                      sizeof(double) * tile_stride, sizeof(double) * tile_n / compute_num,
+                      compute_num);
+
+    // switch buffers
+    bsel = !bsel;
+    ni += tile_n;
+
+    // signal worker and wait for complete
+    snrt_dma_wait_all();
+    snrt_cluster_hw_barrier();
+
+    for (; ni < n; ni += tile_n) {
+
+      // copy-out
+      if (ni > tile_n) {
+        snrt_dma_start_2d(&dy[ni - 2 * tile_n], &l1_dy[bsel * buf_stride],
+                          bpw * tile_n / compute_num, sizeof(double) * tile_n / compute_num,
+                          sizeof(double) * tile_stride, compute_num);
+      }
+
+      // copy-in
+      snrt_dma_start_2d(&l1_dx[bsel * buf_stride], &dx[ni], bpw * tile_n / compute_num,
+                        sizeof(double) * tile_stride, sizeof(double) * tile_n / compute_num,
+                        compute_num);
+      snrt_dma_start_2d(&l1_dy[bsel * buf_stride], &dy[ni], bpw * tile_n / compute_num,
+                        sizeof(double) * tile_stride, sizeof(double) * tile_n / compute_num,
+                        compute_num);
+
+      // switch buffers
+      bsel = !bsel;
+
+      // signal worker and wait for complete
+      snrt_dma_wait_all();
+      snrt_cluster_hw_barrier();
+    }
+
+    // last two copy-out
+    snrt_dma_start_2d(&dy[ni - 2 * tile_n], &l1_dy[bsel * buf_stride], bpw * tile_n / compute_num,
+                      sizeof(double) * tile_n / compute_num, sizeof(double) * tile_stride,
+                      compute_num);
+    bsel = !bsel;
+    snrt_cluster_hw_barrier();
+    snrt_dma_start_2d(&dy[ni - 1 * tile_n], &l1_dy[bsel * buf_stride], bpw * tile_n / compute_num,
+                      sizeof(double) * tile_n / compute_num, sizeof(double) * tile_stride,
+                      compute_num);
+    snrt_dma_wait_all();
+  }
+
+  // ------------------
+  //   Compute
+  // ------------------
+  else {
+    const uint32_t element_start = tile_stride * compute_id;
+    for (ni = 0; ni < n; ni += tile_n) {
+      snrt_cluster_hw_barrier();
+
+      if (ccfg->cycle_start == 0)
+        ccfg->cycle_start = read_csr(mcycle);
+      if (ccfg->max_stmps)
+        ccfg->stmps[--ccfg->max_stmps] = read_csr(mcycle);
+
+      axpy(tile_n / compute_num, l1_alpha, &l1_dx[element_start + bsel * buf_stride],
+           &l1_dy[element_start + bsel * buf_stride], ni == 0);
+
+      ccfg->cycle_end = read_csr(mcycle);
+      if (ccfg->max_stmps)
+        ccfg->stmps[--ccfg->max_stmps] = read_csr(mcycle);
+
+      // switch buffers
+      bsel = !bsel;
+    }
+    snrt_cluster_hw_barrier();
+  }
+
+  // ------------------
+  //   Cleanup
+  // ------------------
+  return;
+}
+
+void __attribute__((noinline))
+axpy(uint32_t n, double *alpha, double *dx, double *dy, uint32_t setup_ssr) {
+  uint32_t compute_id = snrt_cluster_compute_core_idx();
+  uint32_t compute_num = snrt_cluster_compute_core_num();
+
+  double alpha_load = *alpha;
+
+  if (setup_ssr) {
+    __builtin_ssr_setup_1d_r(SNRT_SSR_DM0, 0, n - 1, sizeof(double), dx);
+    __builtin_ssr_setup_1d_r(SNRT_SSR_DM1, 0, n - 1, sizeof(double), dy);
+    __builtin_ssr_setup_1d_w(SNRT_SSR_DM2, 0, n - 1, sizeof(double), dy);
+  } else {
+    __builtin_ssr_read_imm(SNRT_SSR_DM0, 0, dx);
+    __builtin_ssr_read_imm(SNRT_SSR_DM1, 0, dy);
+    __builtin_ssr_write_imm(SNRT_SSR_DM2, 0, dy);
+  }
+
+  snrt_ssr_enable();
+  asm volatile("frep.o %[n_frep], 1, 0, 0 \n"
+               "fmadd.d ft2, %[alpha], ft0, ft1\n"
+               :
+               : [ n_frep ] "r"(n - 1), [ alpha ] "f"(alpha_load)
+               : "ft0", "ft1", "ft2", "memory");
+  snrt_fpu_fence();
+  snrt_ssr_disable();
+}
+
+unsigned test_axpy(void) {
+  uint32_t compute_id = snrt_cluster_compute_core_idx();
+  uint32_t compute_num = snrt_cluster_compute_core_num();
+  const uint32_t axpy_n = 64;
+
+  // allocate data
+  double *ptr = (double *)snrt_cluster_memory().start;
+  double *dx = ptr;
+  ptr += axpy_n;
+  double *dy = ptr;
+  ptr += axpy_n;
+  double *alpha = ptr;
+  ptr += 1;
+
+  // generate data
+  if (compute_id == 0) {
+    *alpha = 2.0;
+    for (unsigned i = 0; i < axpy_n; ++i) {
+      dx[i] = (double)i * 0.5;
+      dy[i] = 100.0;
+    }
+  }
+  snrt_cluster_hw_barrier();
+
+  // call kernel
+  if (snrt_is_compute_core()) {
+    uint32_t element_start = (axpy_n / compute_num) * compute_id;
+    uint32_t element_num = axpy_n / compute_num;
+    if (compute_id == compute_num - 1)
+      element_num += axpy_n - (axpy_n / compute_num * compute_num);
+    axpy(element_num, alpha, &dx[element_start], &dy[element_start], 1);
+  }
+  snrt_cluster_hw_barrier();
+
+  // verify
+  unsigned errors = 0;
+  if (compute_id == 0) {
+    for (unsigned i = 0; i < axpy_n; ++i) {
+      if (dy[i] != 100.0 + (double)i) {
+        printf("%3d mismatch is %.3f should %.3f\r\n", i, dy[i], 100.0 + (double)i);
+        ++errors;
+      }
+    }
+  }
+  return errors;
+}
-- 
2.15.1

