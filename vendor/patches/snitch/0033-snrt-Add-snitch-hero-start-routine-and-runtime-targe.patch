From 472b7cd10f6a6902efc98733aca33b7a6b5e89d1 Mon Sep 17 00:00:00 2001
From: Noah Huetter <noahhuetter@gmail.com>
Date: Mon, 16 May 2022 14:57:18 +0200
Subject: snrt: Add snitch-hero start routine and runtime target

Signed-off-by: Noah Huetter <noahhuetter@gmail.com>
---
 sw/snRuntime/CMakeLists.txt                |   5 +
 sw/snRuntime/include/snitch_hero_support.h | 159 ++++++++++++
 sw/snRuntime/src/snitch_hero_support.c     |  89 +++++++
 sw/snRuntime/src/start_hero.c              | 373 +++++++++++++++++++++++++++++
 4 files changed, 626 insertions(+)
 create mode 100644 sw/vendor/snitch/sw/snRuntime/include/snitch_hero_support.h
 create mode 100644 sw/vendor/snitch/sw/snRuntime/src/snitch_hero_support.c
 create mode 100644 sw/vendor/snitch/sw/snRuntime/src/start_hero.c

diff --git a/sw/snRuntime/CMakeLists.txt b/sw/snRuntime/CMakeLists.txt
index d4e3727..458a99d 100644
--- a/sw/snRuntime/CMakeLists.txt
+++ b/sw/snRuntime/CMakeLists.txt
@@ -108,6 +108,7 @@ if (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)
     # Bare Runtimes (with startup code)
     add_snitch_library(snRuntime-banshee src/start_banshee.S src/start_banshee.c ${sources} src/ssr_v1.c)
     add_snitch_library(snRuntime-cluster src/start_cluster.S src/start_cluster.c ${sources} src/ssr_v2.c)
+    add_snitch_library(snRuntime-hero src/start_cluster.S src/start_hero.c src/snitch_hero_support.c ${sources} src/ssr_v2.c)
     add_snitch_library(snRuntime-billywig src/start_billywig.S ${sources} src/ssr_v1.c)
     add_snitch_library(snRuntime-bowtruckle src/start_bowtruckle.S ${sources} src/ssr_v1.c)
     add_snitch_library(snRuntime-lethifold src/start_lethifold.S ${sources} src/ssr_v1.c)
@@ -136,6 +137,10 @@ else()
         else()
             add_snitch_library(snRuntime-cluster src/start_cluster.S src/start_cluster.c ${sources} src/ssr_v2.c)
         endif()
+    elseif(SNITCH_RUNTIME STREQUAL "snRuntime-hero")
+        # Supported only with the LLVM toolchain
+        add_snitch_library(snRuntime-hero src/start_cluster.S src/start_hero.c src/snitch_hero_support.c ${sources} src/ssr_v3.c)
+        set(public_headers ${public_headers} include/snitch_hero_support.h)
     elseif(SNITCH_RUNTIME STREQUAL "snRuntime-billywig")
         add_snitch_library(snRuntime-billywig src/start_billywig.S ${sources} src/ssr_v1.c)
     elseif(SNITCH_RUNTIME STREQUAL "snRuntime-bowtruckle")
diff --git a/sw/snRuntime/include/snitch_hero_support.h b/sw/snRuntime/include/snitch_hero_support.h
new file mode 100644
index 0000000..0bad1e2
--- /dev/null
+++ b/sw/snRuntime/include/snitch_hero_support.h
@@ -0,0 +1,159 @@
+// Copyright 2020 ETH Zurich and University of Bologna.
+// Licensed under the Apache License, Version 2.0, see LICENSE for details.
+// SPDX-License-Identifier: Apache-2.0
+#pragma once
+
+#include <stdint.h>
+#include <string.h>
+
+/***********************************************************************************
+ * MACROS
+ ***********************************************************************************/
+#define SYS_exit 60
+#define SYS_write 64
+#define SYS_read 63
+#define SYS_wake 1235
+#define SYS_cycle 1236
+
+/***********************************************************************************
+ * TYPES
+ ***********************************************************************************/
+
+/**
+ * @brief Ring buffer for simple communication from accelerator to host.
+ * @tail: Points to the element in `data` which is read next
+ * @head: Points to the element in `data` which is written next
+ * @size: Number of elements in `data`. Head and tail pointer wrap at `size`
+ * @element_size: Size of each element in bytes
+ * @data_p: points to the base of the data buffer in physical address
+ * @data_v: points to the base of the data buffer in virtual address space
+ */
+struct ring_buf {
+  uint32_t tail;
+  uint64_t data_v;
+  uint32_t element_size;
+  uint32_t size;
+  uint64_t data_p;
+  // put accelerator data onto a new cache-line (cva6 specific: 128-bit cache lines)
+  uint8_t _pad1[4];
+  uint32_t head;
+};
+
+/***********************************************************************************
+ * DATA
+ ***********************************************************************************/
+extern volatile struct ring_buf *g_a2h_rb;
+extern volatile struct ring_buf *g_a2h_mbox;
+extern volatile struct ring_buf *g_h2a_mbox;
+
+/***********************************************************************************
+ * INLINES
+ ***********************************************************************************/
+/**
+ * @brief Copy data from `el` in the next free slot in the ring-buffer on the physical addresses
+ *
+ * @param rb pointer to the ring buffer struct
+ * @param el pointer to the data to be copied into the ring buffer
+ * @return int 0 on succes, -1 if the buffer is full
+ */
+static inline int rb_device_put(volatile struct ring_buf *rb, void *el) {
+  uint32_t next_head = (rb->head + 1) % rb->size;
+  // caught the tail, can't put data
+  if (next_head == rb->tail)
+    return -1;
+  for (uint32_t i = 0; i < rb->element_size; i++)
+    *((uint8_t *)rb->data_p + rb->element_size * rb->head + i) = *((uint8_t *)el + i);
+  rb->head = next_head;
+  return 0;
+}
+/**
+ * @brief Pop element from ring buffer on virtual addresses
+ *
+ * @param rb pointer to ring buffer struct
+ * @param el pointer to where element is copied to
+ * @return 0 on success, -1 if no element could be popped
+ */
+static inline int rb_host_get(volatile struct ring_buf *rb, void *el) {
+  // caught the head, can't get data
+  if (rb->tail == rb->head)
+    return -1;
+  for (uint32_t i = 0; i < rb->element_size; i++)
+    *((uint8_t *)el + i) = *((uint8_t *)rb->data_v + rb->element_size * rb->tail + i);
+  rb->tail = (rb->tail + 1) % rb->size;
+  return 0;
+}
+
+/**
+ * @brief Copy data from `el` in the next free slot in the ring-buffer on the virtual addresses
+ *
+ * @param rb pointer to the ring buffer struct
+ * @param el pointer to the data to be copied into the ring buffer
+ * @return int 0 on succes, -1 if the buffer is full
+ */
+static inline int rb_host_put(volatile struct ring_buf *rb, void *el) {
+  uint32_t next_head = (rb->head + 1) % rb->size;
+  // caught the tail, can't put data
+  if (next_head == rb->tail)
+    return -1;
+  for (uint32_t i = 0; i < rb->element_size; i++)
+    *((uint8_t *)rb->data_v + rb->element_size * rb->head + i) = *((uint8_t *)el + i);
+  rb->head = next_head;
+  return 0;
+}
+/**
+ * @brief Pop element from ring buffer on physicl addresses
+ *
+ * @param rb pointer to ring buffer struct
+ * @param el pointer to where element is copied to
+ * @return 0 on success, -1 if no element could be popped
+ */
+static inline int rb_device_get(volatile struct ring_buf *rb, void *el) {
+  // caught the head, can't get data
+  if (rb->tail == rb->head)
+    return -1;
+  for (uint32_t i = 0; i < rb->element_size; i++)
+    *((uint8_t *)el + i) = *((uint8_t *)rb->data_p + rb->element_size * rb->tail + i);
+  rb->tail = (rb->tail + 1) % rb->size;
+  return 0;
+}
+/**
+ * @brief Init the ring buffer. See `struct ring_buf` for details
+ */
+static inline void rb_init(volatile struct ring_buf *rb, uint64_t size, uint64_t element_size) {
+  rb->tail = 0;
+  rb->head = 0;
+  rb->size = size;
+  rb->element_size = element_size;
+}
+
+/**
+ * @brief Holds physical addresses of the shared L3
+ * @a2h_rb: accelerator to host ring buffer
+ * @head: base of heap memory
+ */
+struct l3_layout {
+  uint32_t a2h_rb;
+  uint32_t a2h_mbox;
+  uint32_t h2a_mbox;
+  uint32_t heap;
+};
+
+/***********************************************************************************
+ * PUBLICS
+ ***********************************************************************************/
+int syscall(uint64_t which, uint64_t arg0, uint64_t arg1, uint64_t arg2, uint64_t arg3,
+            uint64_t arg4);
+void csleep(uint32_t cycles);
+void snrt_hero_exit(int code);
+/**
+ * @brief Blocking mailbox read access
+ */
+int snitch_mbox_read(uint32_t *buffer, size_t n_words);
+/**
+ * @brief Non-Blocking mailbox read access. Return 1 on success, 0 on fail
+ */
+int snitch_mbox_try_read(uint32_t *buffer);
+/**
+ * @brief Blocking mailbox write access
+ */
+int snitch_mbox_write(uint32_t word);
diff --git a/sw/snRuntime/src/snitch_hero_support.c b/sw/snRuntime/src/snitch_hero_support.c
new file mode 100644
index 0000000..c508413
--- /dev/null
+++ b/sw/snRuntime/src/snitch_hero_support.c
@@ -0,0 +1,89 @@
+// Copyright 2020 ETH Zurich and University of Bologna.
+// Licensed under the Apache License, Version 2.0, see LICENSE for details.
+// SPDX-License-Identifier: Apache-2.0
+
+#include "snrt.h"
+#include "snitch_hero_support.h"
+
+/***********************************************************************************
+ * MACROS
+ ***********************************************************************************/
+#define SYS_exit 60
+#define SYS_write 64
+#define SYS_read 63
+#define SYS_wake 1235
+#define SYS_cycle 1236
+
+/***********************************************************************************
+ * DATA
+ ***********************************************************************************/
+volatile struct ring_buf *g_a2h_rb;
+volatile struct ring_buf *g_a2h_mbox;
+volatile struct ring_buf *g_h2a_mbox;
+
+/***********************************************************************************
+ * FUNCTIONS
+ ***********************************************************************************/
+void csleep(uint32_t cycles) {
+  uint32_t start = read_csr(mcycle);
+  while ((read_csr(mcycle) - start) < cycles)
+    ;
+}
+
+int syscall(uint64_t which, uint64_t arg0, uint64_t arg1, uint64_t arg2, uint64_t arg3,
+            uint64_t arg4) {
+  uint64_t magic_mem[6];
+  int ret;
+  uint32_t retries = 0;
+
+  volatile struct ring_buf *rb = g_a2h_rb;
+
+  magic_mem[0] = which;
+  magic_mem[1] = arg0;
+  magic_mem[2] = arg1;
+  magic_mem[3] = arg2;
+  magic_mem[4] = arg3;
+  magic_mem[5] = arg4;
+
+  do {
+    ret = rb_device_put(rb, (void *)magic_mem);
+    if (ret) {
+      ++retries;
+      csleep(1000000);
+    }
+  } while (ret != 0);
+  return retries;
+}
+
+void snrt_putchar(char c) { syscall(SYS_write, 1, c, 1, 0, 0); }
+
+void snrt_hero_exit(int code) { syscall(SYS_exit, code, 0, 0, 0, 0); }
+
+/***********************************************************************************
+ * MAILBOX
+ ***********************************************************************************/
+int snitch_mbox_try_read(uint32_t *buffer) {
+  return rb_device_get(g_h2a_mbox, buffer) == 0 ? 1 : 0;
+}
+int snitch_mbox_read(uint32_t *buffer, size_t n_words) {
+  int ret;
+  while (n_words--) {
+    do {
+      ret = rb_device_get(g_h2a_mbox, &buffer[n_words]);
+      if (ret) {
+        csleep(10000);
+      }
+    } while (ret);
+  }
+  return 0;
+}
+int snitch_mbox_write(uint32_t word) {
+  int ret;
+  do {
+    ret = rb_device_put(g_a2h_mbox, &word);
+    if (ret) {
+      csleep(10000);
+    }
+  } while (ret);
+  return ret;
+}
diff --git a/sw/snRuntime/src/start_hero.c b/sw/snRuntime/src/start_hero.c
new file mode 100644
index 0000000..d7d7747
--- /dev/null
+++ b/sw/snRuntime/src/start_hero.c
@@ -0,0 +1,373 @@
+// Copyright 2020 ETH Zurich and University of Bologna.
+// Licensed under the Apache License, Version 2.0, see LICENSE for details.
+// SPDX-License-Identifier: Apache-2.0
+#define DEBUG
+
+#include <stdarg.h>
+
+#include "omp.h"
+#include "printf.h"
+#include "snitch_cluster_peripheral.h"
+#include "snitch_hero_support.h"
+#include "snrt.h"
+#include "team.h"
+#include "dm.h"
+#include "perf_cnt.h"
+
+#include "debug.h"
+
+//================================================================================
+// MACROS AND SETTINGS
+//================================================================================
+
+// set to >0 for debugging
+#define DEBUG_LEVEL_OFFLOAD_MANAGER 1
+
+const uint32_t active_pe = 8;
+
+/* MAILBOX SIGNALING */
+#define MBOX_DEVICE_READY (0x01U)
+#define MBOX_DEVICE_START (0x02U)
+#define MBOX_DEVICE_BUSY (0x03U)
+#define MBOX_DEVICE_DONE (0x04U)
+#define MBOX_DEVICE_STOP (0x0FU)
+#define MBOX_DEVICE_LOGLVL (0x10U)
+#define MBOX_HOST_READY (0x1000U)
+#define MBOX_HOST_DONE (0x3000U)
+
+#define TO_RUNTIME (0x10000000U) // bypass PULP driver
+#define RAB_UPDATE (0x20000000U) // handled by PULP driver
+#define RAB_SWITCH (0x30000000U) // handled by PULP driver
+
+//================================================================================
+// TYPES
+//================================================================================
+
+// Shrinked gomp_team_t descriptor
+typedef struct offload_rab_miss_handler_desc_s {
+  void (*omp_task_f)(void *arg, uint32_t argc);
+  void *omp_args;
+  void *omp_argc;
+  int barrier_id;
+} offload_rab_miss_handler_desc_t;
+
+typedef uint32_t virt_addr_t;
+typedef uint32_t virt_pfn_t;
+
+// This struct represents a miss in the RAB Miss Hardware FIFO.
+typedef struct rab_miss_t {
+  virt_addr_t virt_addr;
+  int core_id;
+  int cluster_id;
+  int intra_cluster_id;
+  uint8_t is_prefetch;
+} rab_miss_t;
+
+//================================================================================
+// Data
+//================================================================================
+static volatile uint32_t g_printf_mutex = 0;
+
+static volatile uint32_t *soc_scratch = (uint32_t *)(0x02000014);
+struct l3_layout l3l;
+
+const uint32_t snrt_stack_size __attribute__((weak, section(".rodata"))) = 12;
+
+// The boot data generated along with the system RTL.
+// See `hw/system/snitch_cluster/test/tb_lib.hh` for details.
+struct snrt_cluster_bootdata {
+  uint32_t boot_addr;
+  uint32_t core_count;
+  uint32_t hartid_base;
+  uint32_t tcdm_start;
+  uint32_t tcdm_size;
+  uint32_t tcdm_offset;
+  uint64_t global_mem_start;
+  uint64_t global_mem_end;
+  uint32_t cluster_count;
+  uint32_t s1_quadrant_count;
+  uint32_t clint_base;
+};
+
+// Rudimentary string buffer for putc calls.
+
+void _snrt_init_team(uint32_t cluster_core_id, uint32_t cluster_core_num, void *spm_start,
+                     void *spm_end, const struct snrt_cluster_bootdata *bootdata,
+                     struct snrt_team_root *team) {
+  (void)cluster_core_id;
+  team->base.root = team;
+  team->bootdata = (void *)bootdata;
+  team->global_core_base_hartid = bootdata->hartid_base;
+  team->global_core_num =
+      bootdata->core_count * bootdata->cluster_count * bootdata->s1_quadrant_count;
+  team->cluster_idx = (snrt_hartid() - bootdata->hartid_base) / bootdata->core_count;
+  team->cluster_num = bootdata->cluster_count * bootdata->s1_quadrant_count;
+  team->cluster_core_base_hartid = bootdata->hartid_base;
+  team->cluster_core_num = cluster_core_num;
+  team->global_mem.start = (uint64_t)(bootdata->global_mem_start);
+  team->global_mem.end = (uint64_t)bootdata->global_mem_end;
+  team->cluster_mem.start = (uint64_t)spm_start;
+  team->cluster_mem.end = (uint64_t)spm_end;
+  team->barrier_reg_ptr =
+      (uint32_t)spm_start + bootdata->tcdm_size + SNITCH_CLUSTER_PERIPHERAL_HW_BARRIER_REG_OFFSET;
+
+  // Initialize cluster barrier
+  team->cluster_barrier.barrier = 0;
+  team->cluster_barrier.barrier_iteration = 0;
+
+  // TLS caches of frequently used data
+  _snrt_team_current = &team->base;
+  _snrt_core_idx = (snrt_hartid() - _snrt_team_current->root->cluster_core_base_hartid) %
+                   _snrt_team_current->root->cluster_core_num;
+
+  // init peripherals
+  team->peripherals.clint = (uint32_t *)bootdata->clint_base;
+  team->peripherals.perf_counters =
+      (uint32_t *)(spm_start + bootdata->tcdm_size +
+                   SNITCH_CLUSTER_PERIPHERAL_PERF_COUNTER_ENABLE_0_REG_OFFSET);
+  team->peripherals.wakeup = (uint32_t *)0; // not supported in RTL anymore
+  team->peripherals.cl_clint = (uint32_t *)(spm_start + bootdata->tcdm_size +
+                                            SNITCH_CLUSTER_PERIPHERAL_CL_CLINT_SET_REG_OFFSET);
+
+  // Init allocator
+  snrt_alloc_init(team, 0);
+  snrt_int_init(team);
+  // TODO: Should be protected by a global barrier
+  g_printf_mutex = 0;
+}
+
+/**
+ * @brief Called by each hart before the pre-main barrier in snrt crt0
+ *
+ */
+void _snrt_hier_wakeup(void) {
+  const uint32_t core_id = snrt_cluster_core_idx();
+
+  // master core wakes other cluster cores through cluster local clint
+  if (core_id == 0) {
+    // clear the interrupt from cva6
+    snrt_int_sw_clear(snrt_hartid());
+    // wake remaining cluster cores
+    const unsigned cluster_core_num = snrt_cluster_core_num();
+    snrt_int_cluster_set(~0x1 & ((1 << cluster_core_num) - 1));
+  } else {
+    // clear my interrupt
+    snrt_int_cluster_clr(1 << core_id);
+  }
+}
+
+//================================================================================
+// TODO: Symbols to declare somewhere else on a merge
+//================================================================================
+/**
+ * @brief A re-entrant wrapper to printf
+ *
+ */
+void snrt_printf(const char *format, ...) {
+  va_list args;
+
+  snrt_mutex_lock(&g_printf_mutex);
+
+  va_start(args, format);
+  vprintf(format, args);
+  va_end(args);
+
+  snrt_mutex_release(&g_printf_mutex);
+}
+
+//================================================================================
+// HERO Functions
+//================================================================================
+
+static void offload_rab_misses_handler(void *arg, uint32_t argc) {
+  (void)arg;
+  (void)argc;
+  snrt_error("unimplemented!\r\n");
+  // static void offload_rab_misses_handler(uint32_t *status) {
+  // uint32_t *status = (uint32_t)arg;
+  // if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+  //   snrt_trace("offload_rab_misses_handler: synch @%p (0x%x)\n", status,
+  //              *(volatile unsigned int *)status);
+  // do {
+  //   handle_rab_misses();
+  // } while (*((volatile uint32_t *)status) != 0xdeadbeefU);
+  // if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+  //   snrt_trace("offload_rab_misses_handler: synch @%p (0x%x)\n", status,
+  //              *(volatile unsigned int *)status);
+}
+
+static int gomp_offload_manager() {
+  const uint32_t core_id = snrt_cluster_core_idx();
+
+  // Init the manager (handshake btw host and accelerator is here)
+  // gomp_init_offload_manager();
+
+  // FIXME For the momenent we are not using the cmd sended as trigger.
+  // It should be used to perform the deactivation of the accelerator,
+  // as well as other operations, like local data allocation or movement.
+  // FIXME Note that the offload at the moment use several time the mailbox.
+  // We should compact the offload descriptor and just sent a pointer to
+  // that descriptor.
+  uint32_t cmd = (uint32_t)NULL;
+  uint32_t data;
+
+  // Offloaded function pointer and arguments
+  void (*offloadFn)(uint64_t) = NULL;
+  uint64_t offloadArgs = 0x0;
+  unsigned nbOffloadRabMissHandlers = 0x0;
+  uint32_t offload_rab_miss_sync = 0x0U;
+  // offload_rab_miss_handler_desc_t rab_miss_handler = {.omp_task_f = offload_rab_misses_handler,
+  //                                                     .omp_args = (void *)&offload_rab_miss_sync,
+  //                                                     .omp_argc = 1,
+  //                                                     .barrier_id = -1};
+
+  int cycles = 0;
+  uint32_t issue_fpu, dma_busy;
+  rab_miss_t rab_miss;
+  // reset_vmm();
+
+  while (1) {
+    if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+      snrt_trace("Waiting for command...\n");
+
+    // (1) Wait for the offload trigger cmd == MBOX_DEVICE_START
+    snitch_mbox_read((unsigned int *)&cmd, 1);
+    if (MBOX_DEVICE_STOP == cmd) {
+      if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+        snrt_trace("Got MBOX_DEVICE_STOP from host, stopping execution now.\n");
+      break;
+    } else if (MBOX_DEVICE_LOGLVL == cmd) {
+      if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+        snrt_trace("Got command 0x%x, setting log level.\n", cmd);
+      snitch_mbox_read((unsigned int *)&data, 1);
+      snrt_debug_set_loglevel(data);
+      continue;
+    } else if (MBOX_DEVICE_START != cmd) {
+      if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+        snrt_trace("Got unexpected command 0x%x, stopping execution now.\n", cmd);
+      break;
+    }
+
+    // (2) The host sends through the mailbox the pointer to the function that should be
+    // executed on the accelerator.
+    snitch_mbox_read((unsigned int *)&offloadFn, 1);
+
+    if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+      snrt_trace("tgt_fn @ 0x%x\n", (unsigned int)offloadFn);
+
+    // (3) The host sends through the mailbox the pointer to the arguments that should
+    // be used.
+    snitch_mbox_read((unsigned int *)&offloadArgs, 1);
+
+    if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+      snrt_trace("tgt_vars @ 0x%x\n", (unsigned int)offloadArgs);
+
+    // (3b) The host sends through the mailbox the number of rab misses handlers threads
+    snitch_mbox_read((unsigned int *)&nbOffloadRabMissHandlers, 1);
+
+    if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+      snrt_trace("nbOffloadRabMissHandlers %d/%d\n", nbOffloadRabMissHandlers, active_pe);
+
+    // (3c) Spawning nbOffloadRabMissHandlers
+    unsigned mhCoreMask = 0;
+    nbOffloadRabMissHandlers =
+        nbOffloadRabMissHandlers < active_pe - 1 ? nbOffloadRabMissHandlers : active_pe - 1;
+    if (nbOffloadRabMissHandlers) {
+      offload_rab_miss_sync = 0x0U;
+      for (int pid = active_pe - 1, i = nbOffloadRabMissHandlers; i > 0; i--, pid--) {
+        if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+          snrt_trace("enabling RAB miss handler on %d\n", pid);
+        mhCoreMask |= (1 << pid);
+      }
+    }
+    omp_getData()->maxThreads = active_pe - nbOffloadRabMissHandlers;
+    omp_getData()->numThreads = active_pe - nbOffloadRabMissHandlers;
+    // eu_dispatch_team_config(mhCoreMask);
+    // eu_dispatch_push((unsigned int)&offload_rab_misses_handler);
+    // eu_dispatch_push((unsigned int)&offload_rab_miss_sync);
+    // eu_dispatch_team_config(omp_getData()->coreMask);
+
+    // (4) Ensure access to offloadArgs. It might be in SVM.
+    if (offloadArgs != 0x0) {
+      // FIXME
+      // pulp_tryread((unsigned int *)offloadArgs);
+    }
+    if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+      snrt_trace("begin offloading\n");
+    // reset_timer();
+    // start_timer();
+
+    for (unsigned i = 0; i < 10; i += 2) {
+      snrt_trace(" %2d: 0x%08x = %08d  %2d: 0x%08x = %08d\n", i, ((uint32_t *)offloadArgs)[i],
+                 *((uint32_t *)(((uint32_t *)offloadArgs)[i])), i + 1,
+                 ((uint32_t *)offloadArgs)[i + 1],
+                 *((uint32_t *)(((uint32_t *)offloadArgs)[i + 1])));
+    }
+
+    // (5) Execute the offloaded function.
+    snrt_reset_perf_counter(SNRT_PERF_CNT0);
+    snrt_reset_perf_counter(SNRT_PERF_CNT1);
+    snrt_start_perf_counter(SNRT_PERF_CNT0, SNRT_PERF_CNT_ISSUE_FPU, core_id);
+    snrt_start_perf_counter(SNRT_PERF_CNT1, SNRT_PERF_CNT_DMA_BUSY, core_id);
+    cycles = read_csr(mcycle);
+
+    offloadFn(offloadArgs);
+    
+    cycles = read_csr(mcycle) - cycles;
+    snrt_stop_perf_counter(SNRT_PERF_CNT0);
+    snrt_stop_perf_counter(SNRT_PERF_CNT1);
+    issue_fpu = snrt_get_perf_counter(SNRT_PERF_CNT0);
+    dma_busy = snrt_get_perf_counter(SNRT_PERF_CNT1);
+
+    if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+      snrt_trace("end offloading\n");
+    
+    // (6) Report EOC and profiling
+    snrt_info("cycles: %d issue_fpu: %d dma_busy: %d\r\n", cycles, issue_fpu, dma_busy);
+
+    snitch_mbox_write(MBOX_DEVICE_DONE);
+    snitch_mbox_write(cycles);
+
+    if (DEBUG_LEVEL_OFFLOAD_MANAGER > 0)
+      snrt_trace("Kernel execution time [Snitch cycles] = %d\n", cycles);
+
+    if (nbOffloadRabMissHandlers) {
+      offload_rab_miss_sync = 0xdeadbeefU;
+      // gomp_atomic_add_thread_pool_idle_cores(nbOffloadRabMissHandlers);
+    }
+  }
+
+  return 0;
+}
+
+int main(int argc, char *argv[]) {
+  (void)argc;
+  (void)argv;
+  unsigned core_idx = snrt_cluster_core_idx();
+  unsigned core_num = snrt_cluster_core_num();
+
+  /**
+   * One core initializes the global data structures
+   */
+  if (snrt_is_dm_core()) {
+    // read memory layout from scratch2
+    memcpy(&l3l, (void *)soc_scratch[2], sizeof(struct l3_layout));
+    g_a2h_rb = (struct ring_buf *)l3l.a2h_rb;
+    g_a2h_mbox = (struct ring_buf *)l3l.a2h_mbox;
+    g_h2a_mbox = (struct ring_buf *)l3l.h2a_mbox;
+    snrt_trace("LL init done. core_idx: %d core_num: %d\n", core_idx, core_num);
+  }
+  snrt_cluster_hw_barrier();
+
+  __snrt_omp_bootstrap(core_idx);
+
+  snrt_trace("omp_bootstrap complete, core_idx: %d core_num: %d\n", core_idx, core_num);
+
+  gomp_offload_manager();
+
+  snrt_trace("bye\n");
+  // exit
+  __snrt_omp_destroy(core_idx);
+  snrt_hero_exit(0);
+  return 0;
+}
-- 
2.16.5

